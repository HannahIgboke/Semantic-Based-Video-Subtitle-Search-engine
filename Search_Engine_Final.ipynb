{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FINAL CODE STEPS FOR SEARCH ENGINE PROJECT"
      ],
      "metadata": {
        "id": "acCDQptob2ry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Reading the data from the database"
      ],
      "metadata": {
        "id": "xVGzMaJHcJ8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1"
      ],
      "metadata": {
        "id": "XIi_buxqcoN7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKXyCgxeiBkX"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHbG-UqDLsuw"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect(\"Data/eng_subtitles_database.db\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "DLxZqDmFYwzs",
        "outputId": "dbc90f88-3c7b-41dc-cc7f-f7a88ca1101b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num</th>\n",
              "      <th>name</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9180533</td>\n",
              "      <td>the.message.(1976).eng.1cd</td>\n",
              "      <td>b'PK\\x03\\x04\\x14\\x00\\x00\\x00\\x08\\x00\\x1c\\xa9\\x...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9180583</td>\n",
              "      <td>here.comes.the.grump.s01.e09.joltin.jack.in.bo...</td>\n",
              "      <td>b'PK\\x03\\x04\\x14\\x00\\x00\\x00\\x08\\x00\\x17\\xb9\\x...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9180592</td>\n",
              "      <td>yumis.cells.s02.e13.episode.2.13.(2022).eng.1cd</td>\n",
              "      <td>b'PK\\x03\\x04\\x14\\x00\\x00\\x00\\x08\\x00L\\xb9\\x99V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9180594</td>\n",
              "      <td>yumis.cells.s02.e14.episode.2.14.(2022).eng.1cd</td>\n",
              "      <td>b'PK\\x03\\x04\\x14\\x00\\x00\\x00\\x08\\x00U\\xa9\\x99V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9180600</td>\n",
              "      <td>broker.(2022).eng.1cd</td>\n",
              "      <td>b'PK\\x03\\x04\\x14\\x00\\x00\\x00\\x08\\x001\\xa9\\x99V...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       num                                               name  \\\n",
              "0  9180533                         the.message.(1976).eng.1cd   \n",
              "1  9180583  here.comes.the.grump.s01.e09.joltin.jack.in.bo...   \n",
              "2  9180592    yumis.cells.s02.e13.episode.2.13.(2022).eng.1cd   \n",
              "3  9180594    yumis.cells.s02.e14.episode.2.14.(2022).eng.1cd   \n",
              "4  9180600                              broker.(2022).eng.1cd   \n",
              "\n",
              "                                             content  \n",
              "0  b'PK\\x03\\x04\\x14\\x00\\x00\\x00\\x08\\x00\\x1c\\xa9\\x...  \n",
              "1  b'PK\\x03\\x04\\x14\\x00\\x00\\x00\\x08\\x00\\x17\\xb9\\x...  \n",
              "2  b'PK\\x03\\x04\\x14\\x00\\x00\\x00\\x08\\x00L\\xb9\\x99V...  \n",
              "3  b'PK\\x03\\x04\\x14\\x00\\x00\\x00\\x08\\x00U\\xa9\\x99V...  \n",
              "4  b'PK\\x03\\x04\\x14\\x00\\x00\\x00\\x08\\x001\\xa9\\x99V...  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "query = 'SELECT * FROM zipfiles'\n",
        "\n",
        "df = pd.read_sql_query(query, conn)\n",
        "\n",
        "conn.close()\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ktwaq8CYw2c",
        "outputId": "65847ce2-0132-4e13-80de-9a231236e67e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(82498, 3)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NkusL80Yw5V",
        "outputId": "870f54de-ca78-4c2e-cda8-b2d52ddce155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 82498 entries, 0 to 82497\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   num      82498 non-null  int64 \n",
            " 1   name     82498 non-null  object\n",
            " 2   content  82498 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 1.9+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2"
      ],
      "metadata": {
        "id": "0kQr9vQedycJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fauwL-dYw8W"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm, tqdm_notebook\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCzY537MYw_K"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import io\n",
        "def decomp_decode(data):\n",
        "    with zipfile.ZipFile(io.BytesIO(data)) as zip_file:\n",
        "        # Extract the first file in the ZIP archive\n",
        "        file_list = zip_file.namelist()\n",
        "        first_file = file_list[0]\n",
        "        decompressed_data = zip_file.read(first_file)\n",
        "    return decompressed_data.decode('latin-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzkd5-JhY8VK",
        "outputId": "a73ad224-09d7-4465-c84b-a6133df978d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████| 82498/82498 [00:26<00:00, 3125.53it/s]\n"
          ]
        }
      ],
      "source": [
        "df['content'] = df['content'].progress_apply(lambda x : decomp_decode(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5y41QnolY6L2",
        "outputId": "d4046e1a-1403-4334-eb95-118e7334a07f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    1\\r\\n00:00:06,000 --> 00:00:12,074\\r\\nWatch an...\n",
              "1    1\\r\\n00:00:29,359 --> 00:00:32,048\\r\\nAh! Ther...\n",
              "2    1\\r\\n00:00:53,200 --> 00:00:56,030\\r\\n<i>Yumi'...\n",
              "3    1\\r\\n00:00:06,000 --> 00:00:12,074\\r\\nWatch an...\n",
              "4    ï»¿1\\r\\n00:00:06,000 --> 00:00:12,074\\r\\nWatch...\n",
              "Name: content, dtype: object"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['content'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Data cleaning"
      ],
      "metadata": {
        "id": "-DDLQJpUfj9Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O01BIUy0Y8fO"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_data(data): # data is the entire text file entry in the dataframe\n",
        "\n",
        "    # removing timestamps\n",
        "    data = re.sub(\"\\d{2}:\\d{2}:\\d{2},\\d{3}\\s-->\\s\\d{2}:\\d{2}:\\d{2},\\d{3}\",\" \",  data)\n",
        "\n",
        "    # removing index no. of dialogues\n",
        "    data = re.sub(r'\\n?\\d+\\r', \"\", data)\n",
        "\n",
        "    # removing escape sequences like \\n \\r\n",
        "    data = re.sub('\\r|\\n', \"\", data)\n",
        "\n",
        "    # removing <i> and </i>\n",
        "    data = re.sub('<i>|</i>', \"\", data)\n",
        "    # removing links\n",
        "    data = re.sub(\"(?:www\\.)osdb\\.link\\/[\\w\\d]+|www\\.OpenSubtitles\\.org|osdb\\.link\\/ext|api\\.OpenSubtitles\\.org|OpenSubtitles\\.com\", \" \",data)\n",
        "\n",
        "    # Converting to lower case\n",
        "    data = data.lower()\n",
        "\n",
        "    # return\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44yLCfd8YxB9",
        "outputId": "629595a6-494d-47eb-a2c9-d512e91f6351"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████| 82498/82498 [04:54<00:00, 280.38it/s]\n"
          ]
        }
      ],
      "source": [
        "df['content'] = df['content'].progress_apply(lambda x: clean_data(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTlNu-rBY6L3"
      },
      "outputs": [],
      "source": [
        "df['num'] = df['num'].apply(lambda x : str(x)) #converting 'num' id to string as chromaDB ids need to be in string format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0YJO4TIP5Na"
      },
      "source": [
        "## Step 3: Data chunking - semantic chunking"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Creating the chunking function"
      ],
      "metadata": {
        "id": "UmoNB5SlxPFX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU-1zMelZHBm",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv_3n20YjphY"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "model_name = 'paraphrase-MiniLM-L3-v2' #all-MiniLM-L6-v2\n",
        "model = SentenceTransformer(model_name, device='cuda')\n",
        "\n",
        "def semantic_chunking(document, similarity_threshold=0.9):\n",
        "\n",
        "    # Tokenize the document into sentences\n",
        "    sentences = document.split('.')\n",
        "\n",
        "    # Initialize variables for semantic chunks\n",
        "    chunks = []\n",
        "    current_chunk = sentences[0]\n",
        "\n",
        "    # Generate embeddings for the sentences\n",
        "    sentence_embeddings = model.encode(sentences)\n",
        "\n",
        "    # Iterate over the sentences and group semantically similar sentences into chunks\n",
        "    for i in range(1, len(sentences)):\n",
        "        # Calculate cosine similarity between the current sentence and the previous sentence\n",
        "        similarity_score = np.dot(sentence_embeddings[i], sentence_embeddings[i-1]) / (np.linalg.norm(sentence_embeddings[i]) * np.linalg.norm(sentence_embeddings[i-1]))\n",
        "\n",
        "        # If similarity score is above the threshold, add the sentence to the current chunk\n",
        "        if similarity_score >= similarity_threshold:\n",
        "            current_chunk += '.' + sentences[i]\n",
        "        else:\n",
        "            # If similarity score is below the threshold, start a new chunk\n",
        "            chunks.append(current_chunk)\n",
        "            current_chunk = sentences[i]\n",
        "\n",
        "    # Add the last chunk\n",
        "    chunks.append(current_chunk)\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Running the function in batches"
      ],
      "metadata": {
        "id": "nhRWsa1axVjJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSd3OfGzY6L4"
      },
      "outputs": [],
      "source": [
        "# temporary dataframes to split the data into two parts\n",
        "\n",
        "temp_1 = pd.DataFrame()\n",
        "temp_2 = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqPKoLllY6L4"
      },
      "outputs": [],
      "source": [
        "temp_1['num']=df['num'][:30000]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_2['num']=df['num'][30000:]"
      ],
      "metadata": {
        "id": "bzmUOYopZXRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE10hzHdY6L4",
        "outputId": "08d9f292-4878-45b0-b57c-88271d3bc7fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total time in seconds = 4418.998434782028\n"
          ]
        }
      ],
      "source": [
        "# 1st section using joblib for parallel processing on the first part of the data\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "import time\n",
        "start = time.time()\n",
        "temp_1['chunks'] = Parallel(n_jobs=-1)(delayed(semantic_chunking)(item) for item in df['content'].values[:30000])\n",
        "end=time.time()\n",
        "print(f\"Total time in seconds = {end-start}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#saving to json file\n",
        "\n",
        "temp_1.to_json(\"database.json\") #saving data to json file to restrart the kernel and save RAM"
      ],
      "metadata": {
        "id": "KDcM9joaZ4Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DY2ZZ4_Y6L5",
        "outputId": "22ee319f-eaf3-47b7-f47e-5a52da1e492e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total time in seconds = 8644.886986494064\n"
          ]
        }
      ],
      "source": [
        "# 2nd section - using joblib for parallel processing on the second part of the data\n",
        "from joblib import Parallel, delayed\n",
        "import time\n",
        "start = time.time()\n",
        "temp_2['chunks'] = Parallel(n_jobs=-1)(delayed(semantic_chunking)(item) for item in df['content'].values[30000:])\n",
        "end=time.time()\n",
        "print(f\"Total time in seconds = {end-start}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXOUK5U6Y6L5"
      },
      "outputs": [],
      "source": [
        "temp_2.to_json(\"database_p2.json\") #saving data to json file to restrart the kernel and save RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khLuxzbyY6L5"
      },
      "outputs": [],
      "source": [
        "# restarting the kernel\n",
        "# interacting with each part of the json file\n",
        "\n",
        "import json\n",
        "\n",
        "json_file_path = \"database.json\" #database_p2.json\n",
        "with open(json_file_path, 'r') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "df = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Generating text embeddings"
      ],
      "metadata": {
        "id": "EFrDaS5LOw7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Preparing the indexes"
      ],
      "metadata": {
        "id": "3xgfAq72YWed"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdmSBIIVY6L3"
      },
      "outputs": [],
      "source": [
        "#creating index for the data\n",
        "\n",
        "def indexer(item):\n",
        "    index=[]\n",
        "    temp=int(df[df['num']==item].index[0])\n",
        "    for j in range(len(df['chunks'].iloc[temp])):\n",
        "        index.append(item+\"-\"+str(j))# since id needs to be unique adding the j index with a hyphen to create a unique id\n",
        "    return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnppBBFZY6L3"
      },
      "outputs": [],
      "source": [
        "df['num_list'] = df['num'].apply(lambda x : indexer(x)) #indexing the embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Creating the text embeddings"
      ],
      "metadata": {
        "id": "NUG2lC-hYaah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_name = 'paraphrase-MiniLM-L3-v2' #all-MiniLM-L6-v2\n",
        "model = SentenceTransformer(model_name, device='cuda')"
      ],
      "metadata": {
        "id": "o6qYstWxdo67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynOHeAwdY6L5"
      },
      "outputs": [],
      "source": [
        "def embedding_gen(data):\n",
        "  return model.encode(data).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bDf_yvTY6L5"
      },
      "outputs": [],
      "source": [
        "df['embeddings'] = Parallel(n_jobs=-1)(delayed(embedding_gen)(item) for item in df['chunks'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5:  Storing data in ChromaDB"
      ],
      "metadata": {
        "id": "_ksSpf9SUQx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up chromaDB"
      ],
      "metadata": {
        "id": "dIpWR8x--RFk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC6oxVnbY6L5"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "client = chromadb.PersistentClient(path=\"E://search_engine_db\")\n",
        "collection = client.get_or_create_collection(name=\"search_engine\", metadata={\"hnsw:space\": \"cosine\"})\n",
        "collection_2 = client.get_or_create_collection(name=\"search_engine_FileName\", metadata={\"hnsw:space\": \"cosine\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating function to add filenames of our subtitles"
      ],
      "metadata": {
        "id": "_SGcGQpwD7ME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ran this part already before splitting data into 2 temporary dataframes\n",
        "def add_func_v1():\n",
        "    for i in range(df.shape[0]): #setting the range as total no. of rows in dataframe\n",
        "        collection_2.add(\n",
        "            documents=[df['name'].iloc[i]], # adding each filename\n",
        "            embeddings=[[1,2,34,45]], # adding a random data, as we don't need it when retrieving file_name\n",
        "            ids=[df['num'].iloc[i]] # entering unique 'num' id\n",
        "        )"
      ],
      "metadata": {
        "id": "zjLmZnC-mRDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating function to add the chunks, embeddings and unique identifiers for our subtitle files"
      ],
      "metadata": {
        "id": "NgH5WQM6EEhk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdTfIo3qY6L5"
      },
      "outputs": [],
      "source": [
        "def add_func_v2():\n",
        "    for i in range(df.shape[0]): #setting the range as total no. of rows in dataframe\n",
        "        collection.add(\n",
        "            documents=df['chunks'].iloc[i], # adding each chunk\n",
        "            embeddings=df['embeddings'].iloc[i], # adding the corresponding chunk embedding\n",
        "            ids=df['num_list'].iloc[i] #entering the unique 'num' id\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time add_func_v1()"
      ],
      "metadata": {
        "id": "TBcg36NonAzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXSQFt9hY6L5"
      },
      "outputs": [],
      "source": [
        "%time add_func_v2()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Creating the streamlit app"
      ],
      "metadata": {
        "id": "aqBrv6DIHxVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# streamlit code\n",
        "# command to run : streamlit run app.py"
      ],
      "metadata": {
        "id": "1iILj6OBJen2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Importing the necessary libraries"
      ],
      "metadata": {
        "id": "bl0szKjwJh9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import chromadb\n",
        "from sentence_transformers import  SentenceTransformer\n",
        "import streamlit as st"
      ],
      "metadata": {
        "id": "WS0VI0_vJgyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Initializing chromaDB\n"
      ],
      "metadata": {
        "id": "-u46uGgwJqgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = chromadb.PersistentClient(path=\"E://search_engine_db\") #_test_db\n",
        "collection = client.get_collection(name=\"search_engine\") #test_collection\n",
        "collection_name = client.get_collection(name=\"search_engine_FileName\")\n",
        "model_name=\"paraphrase-MiniLM-L3-V2\"\n",
        "model = SentenceTransformer(model_name, device=\"cuda\")"
      ],
      "metadata": {
        "id": "n-HhpTqFJ1K2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3: Cleaning steps for the user query"
      ],
      "metadata": {
        "id": "Js-Y59dnKJVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(data): # data is the query text\n",
        "\n",
        "    # removing timestamps\n",
        "    data = re.sub(\"\\d{2}:\\d{2}:\\d{2},\\d{3}\\s-->\\s\\d{2}:\\d{2}:\\d{2},\\d{3}\",\" \",  data)\n",
        "\n",
        "    # removing index no. of dialogues\n",
        "    data = re.sub(r'\\n?\\d+\\r', \"\", data)\n",
        "\n",
        "    # removing escape sequences like \\n \\r\n",
        "    data = re.sub('\\r|\\n', \"\", data)\n",
        "\n",
        "    # removing <i> and </i>\n",
        "    data = re.sub('<i>|</i>', \"\", data)\n",
        "    # removing links\n",
        "    data = re.sub(\"(?:www\\.)osdb\\.link\\/[\\w\\d]+|www\\.OpenSubtitles\\.org|osdb\\.link\\/ext|api\\.OpenSubtitles\\.org|OpenSubtitles\\.com\", \" \",data)\n",
        "\n",
        "    # Converting to lower case\n",
        "    data = data.lower()\n",
        "\n",
        "    # return\n",
        "    return data"
      ],
      "metadata": {
        "id": "SNfHvXfqKI5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 4: Creating a function to extract the subtitle_id"
      ],
      "metadata": {
        "id": "LyfdadKfKQr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_id(id_list):\n",
        "    new_id_list=[]\n",
        "    for item in id_list:\n",
        "        match = re.match(r'^(\\d+)', item)\n",
        "        if match:\n",
        "            extracted_number = match.group(1)\n",
        "            new_id_list.append(extracted_number)\n",
        "    return new_id_list"
      ],
      "metadata": {
        "id": "viOUhHWOKctB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 5: Creating the web application"
      ],
      "metadata": {
        "id": "8g-W3XdgKgFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "st.header(\"Movie Subtitle Search Engine\")\n",
        "search_query=st.text_input(\"Enter a dialogue to search....\")\n",
        "if st.button(\"Search\")==True:\n",
        "\n",
        "    st.subheader(\"Relevant Subtitle Files\")\n",
        "    search_query=clean_data(search_query)\n",
        "    query_embed = model.encode(search_query).tolist()\n",
        "\n",
        "    search_results=collection.query(query_embeddings=query_embed, n_results=10)\n",
        "    id_list = search_results['ids'][0]\n",
        "\n",
        "    id_list = extract_id(id_list)\n",
        "    print(id_list)\n",
        "    for id in id_list:\n",
        "        file_name = collection_name.get(ids=f\"{id}\")[\"documents\"][0]\n",
        "        st.markdown(f\"[{file_name}](https://www.opensubtitles.org/en/subtitles/{id})\")\n"
      ],
      "metadata": {
        "id": "Zx5bxz_Z3LUq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}